{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaed658b",
   "metadata": {},
   "source": [
    "# Training With Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256c0e3",
   "metadata": {},
   "source": [
    "## !! Context Training is not working properly in this version !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae31f9",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f1f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from LSTM_context import *\n",
    "import torch.nn as nn\n",
    "import Tmaze_Context_2 as TMC\n",
    "import plotting_functions as pf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "\n",
    "importlib.reload(pf)\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b349e4e",
   "metadata": {},
   "source": [
    "## Device Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0557224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_same_device(model, *tensors):\n",
    "    \"\"\"Moves all tensors to the same device as the model.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    return [t.to(device) for t in tensors]\n",
    "\n",
    "def print_device_info(*tensors, model=None):\n",
    "    \"\"\"Prints device information for debugging.\"\"\"\n",
    "    if model:\n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    for i, t in enumerate(tensors):\n",
    "        print(f\"Tensor {i} device: {t.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb2fb2",
   "metadata": {},
   "source": [
    "## Train on Multi/Uni with context and trial counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd48d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model device: cuda:0\n",
      "Input tensor device: cuda:0\n",
      "Target tensor device: cuda:0\n",
      "Mask tensor device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Runyan1\\AppData\\Local\\anaconda3\\envs\\RNN\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/300............. mse_z: 0.0663\n",
      "Epoch: 10/300............. mse_z: 0.0610\n",
      "Epoch: 15/300............. mse_z: 0.0564\n",
      "Epoch: 20/300............. mse_z: 0.0464\n",
      "Epoch: 25/300............. mse_z: 0.0420\n",
      "Epoch: 30/300............. mse_z: 0.0409\n",
      "Epoch: 35/300............. mse_z: 0.0383\n",
      "Epoch: 40/300............. mse_z: 0.0281\n",
      "Epoch: 45/300............. mse_z: 0.0120\n",
      "Epoch: 50/300............. mse_z: 0.0082\n",
      "Epoch: 55/300............. mse_z: 0.0069\n",
      "Epoch: 60/300............. mse_z: 0.0060\n",
      "Epoch: 65/300............. mse_z: 0.0059\n",
      "Epoch: 70/300............. mse_z: 0.0053\n",
      "Epoch: 75/300............. mse_z: 0.0054\n",
      "Epoch: 80/300............. mse_z: 0.0060\n",
      "Epoch: 85/300............. mse_z: 0.0051\n",
      "Epoch: 90/300............. mse_z: 0.0048\n",
      "Epoch: 95/300............. mse_z: 0.0056\n",
      "Epoch: 100/300............. mse_z: 0.0045\n",
      "Epoch: 105/300............. mse_z: 0.0044\n",
      "Epoch: 110/300............. mse_z: 0.0042\n",
      "Epoch: 115/300............. mse_z: 0.0045\n",
      "Epoch: 120/300............. mse_z: 0.0042\n",
      "Epoch: 125/300............. mse_z: 0.0053\n",
      "Epoch: 130/300............. mse_z: 0.0042\n",
      "Epoch: 135/300............. mse_z: 0.0041\n",
      "Epoch: 140/300............. mse_z: 0.0038\n",
      "Epoch: 145/300............. mse_z: 0.0041\n",
      "Epoch: 150/300............. mse_z: 0.0039\n",
      "Epoch: 155/300............. mse_z: 0.0050\n",
      "Epoch: 160/300............. mse_z: 0.0050\n",
      "Epoch: 165/300............. mse_z: 0.0041\n",
      "Epoch: 170/300............. mse_z: 0.0045\n",
      "Epoch: 175/300............. mse_z: 0.0052\n",
      "Epoch: 180/300............. mse_z: 0.0041\n",
      "Epoch: 185/300............. mse_z: 0.0048\n",
      "Epoch: 190/300............. mse_z: 0.0051\n",
      "Epoch: 195/300............. mse_z: 0.0061\n",
      "Epoch: 200/300............. mse_z: 0.0047\n",
      "Epoch: 205/300............. mse_z: 0.0050\n",
      "Epoch: 210/300............. mse_z: 0.0051\n",
      "Epoch: 215/300............. mse_z: 0.0056\n",
      "Epoch: 220/300............. mse_z: 0.0056\n",
      "Epoch: 225/300............. mse_z: 0.0054\n",
      "Epoch: 230/300............. mse_z: 0.0051\n",
      "Epoch: 235/300............. mse_z: 0.0054\n",
      "Epoch: 240/300............. mse_z: 0.0058\n",
      "Epoch: 245/300............. mse_z: 0.0059\n",
      "Epoch: 250/300............. mse_z: 0.0056\n",
      "Epoch: 255/300............. mse_z: 0.0077\n",
      "Epoch: 260/300............. mse_z: 0.0066\n",
      "Epoch: 265/300............. mse_z: 0.0049\n",
      "Epoch: 270/300............. mse_z: 0.0056\n",
      "Epoch: 275/300............. mse_z: 0.0056\n",
      "Epoch: 280/300............. mse_z: 0.0052\n",
      "Epoch: 285/300............. mse_z: 0.0049\n",
      "Epoch: 290/300............. mse_z: 0.0050\n",
      "Epoch: 295/300............. mse_z: 0.0051\n",
      "Epoch: 300/300............. mse_z: 0.0057\n",
      "\n",
      "Input tensor shape: torch.Size([10000, 75, 6])\n",
      "Target tensor shape: torch.Size([10000, 75, 2])\n",
      "Mask tensor shape: torch.Size([10000, 75, 2])\n",
      "\n",
      "Visual trials: 5000\n",
      "Audio trials: 5000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(TMC)\n",
    "\n",
    "# Set device and print for verification\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tmaze = TMC.TmazeEnvContext()\n",
    "# Generate trials with context and trial counter\n",
    "u, z, mask, conditions_train = tmaze.generate_trials(\n",
    "    n_trials=10000,\n",
    "    alpha=0.2,\n",
    "    sigma_in=0.01,\n",
    "    baseline=0.2,\n",
    "    n_coh=15,\n",
    "    trials_per_context=50,\n",
    "    modality='multi',\n",
    "    add_context=True,  # Explicitly add context input\n",
    "    add_trial_counter=True  # Explicitly add trial counter input\n",
    ")\n",
    "\n",
    "# No need to convert to tensor again as generate_trials already returns tensors\n",
    "u = u.to(device)\n",
    "z = z.to(device)\n",
    "mask = mask.to(device)\n",
    "\n",
    "# Create and move model to GPU\n",
    "input_size = u.shape[2]  \n",
    "net = Net_lstm(n=100, input_size=input_size, dale=True, sigma_rec=0.15)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net.lstm.flatten_parameters()\n",
    "net = net.to(device)\n",
    "\n",
    "# Verify device placement\n",
    "print(f\"Model device: {next(net.parameters()).device}\")\n",
    "print(f\"Input tensor device: {u.device}\")\n",
    "print(f\"Target tensor device: {z.device}\")\n",
    "print(f\"Mask tensor device: {mask.device}\")\n",
    "\n",
    "# Train the network\n",
    "net.fit(u, z, mask, \n",
    "        lr=0.0001,  # Learning rate for unimodal training\n",
    "        epochs=300, \n",
    "        verbose=True, \n",
    "        weight_decay=0.001)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\\nInput tensor shape: {u.shape}\")\n",
    "print(f\"Target tensor shape: {z.shape}\")\n",
    "print(f\"Mask tensor shape: {mask.shape}\")\n",
    "\n",
    "# Count trials by context\n",
    "visual_count = sum(1 for condition in conditions_train if condition['context'] == 'visual')\n",
    "audio_count = sum(1 for condition in conditions_train if condition['context'] == 'audio')\n",
    "\n",
    "print(f\"\\nVisual trials: {visual_count}\")\n",
    "print(f\"Audio trials: {audio_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7e4cc",
   "metadata": {},
   "source": [
    "## Check inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "337a54a1",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
